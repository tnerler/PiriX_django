# retrieve_and_generate.py
from typing_extensions import List, TypedDict
from langchain.schema import Document
from utils._faiss import build_store
from utils.load_docs import load_docs
from utils.openai_clients import get_llm
from operator import itemgetter
from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate
from sentence_transformers import CrossEncoder
from utils.chat_history_store import get_session_history
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from utils.summarizer import summarize_messages

cross_encoder = CrossEncoder("cross-encoder/ms-marco-TinyBERT-L-6", device="cuda")

class State(TypedDict):
    question: str
    context: str
    answer: str

def build_chatbot():  
    docs = load_docs()
    vector_store = build_store(docs)

    template = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(
            """
            Sen COMPRU BiliÅŸim KulÃ¼bÃ¼ Tuana Erler, Burcu Kizir, Salih Birdal tarafÄ±ndan tasarlanan Yapay zeka asistanÄ±, PiriX'sin, Piri Reis Ãœniversitesi'nin bilgi asistanÄ±sÄ±n. Temel gÃ¶revin: Okul hakkÄ±nda kÄ±sa, doÄŸru ve anlaÅŸÄ±lÄ±r bilgiler vermek.

            Ã–NEMLÄ° KURALLAR:
            1. SADECE Piri Reis Ãœniversitesi konularÄ±na yanÄ±t ver. DiÄŸer konularda: "Ben sadece Piri Reis Ãœniversitesi hakkÄ±nda bilgi verebilirim ğŸ’™ DiÄŸer konular iÃ§in baÅŸka bir asistana sormanÄ± Ã¶neririm!"
            2. BilmediÄŸin konularda: "Bu konuda ÅŸu anda elimde bilgi yok. DetaylÄ± bilgi iÃ§in Ã§aÄŸrÄ± merkezimizi arayabilirsiniz: +90 216 581 00 50"
            3. Samimi ve arkadaÅŸÃ§a konuÅŸ, robot gibi yanÄ±tlardan kaÃ§Ä±n. Emoji kullanabilirsin ğŸ˜Š
            4. Fiyat bilgilerinde ÅŸunu ekle: "Daha fazla detay iÃ§in: https://aday.pirireis.edu.tr/ucretler/"
            5. BÃ¶lÃ¼m/kulÃ¼p listeleri sorulursa, verilen bilgilere sadÄ±k kalarak numaralÄ± liste kullan. Uydurma.
            6. Okul tanÄ±tÄ±mÄ± sorularÄ±nda gÃ¼Ã§lÃ¼ yÃ¶nleri vurgula ama abartma.
            7. YanÄ±tlar her zaman doÄŸru, kÄ±sa ve net olmalÄ±.
            8. 'Okulun resmi web sitesinden (https://www.pirireis.edu.tr/) ve sosyal medya hesaplarÄ±ndan (https://www.instagram.com/pirireisuni/) bilgi alabilirsin.' diyebilirsin.
            9. RektÃ¶r sorulursa: "RektÃ¶rÃ¼ Ã¶verken, onun liderlik Ã¶zelliklerini ve Ã¼niversiteye katkÄ±larÄ±nÄ± vurgula"
            10. **Ã–NEMLÄ°**: Ã–nceki konuÅŸma geÃ§miÅŸini dikkate al ve konu baÄŸlamÄ±nÄ± koru. KullanÄ±cÄ± daha Ã¶nce bir konu hakkÄ±nda soru sorduysa, yeni sorularÄ±nÄ± o baÄŸlamda deÄŸerlendir.
            11. **TEKRAR ETME**: AynÄ± cevabÄ± tekrar verme, her mesaj benzersiz olmalÄ±.
            12. Tercih indirimi sorulursa: "Tercih indirimleri her yÄ±l geÃ§erli olur."
            13. Burslarla ilgili sorularda: "Burslar hakkÄ±nda detaylÄ± bilgi iÃ§in: https://aday.pirireis.edu.tr/burslar/"
            """
        ),
        MessagesPlaceholder(variable_name="history"),
        HumanMessagePromptTemplate.from_template(
            "BaÄŸlam (Ã–zet): {context}\n\nSoru: {question}\n\nCevap:"
        ),
    ])

    chain = template | get_llm()
    
    chain_with_history = RunnableWithMessageHistory(
        chain,
        get_session_history,
        input_messages_key="question",
        history_messages_key="history"
    )

    def get_conversation_summary(history, n=5):
        """Son n mesajdan ozet cikarir."""
        
        if not history.messages:
            return ""
        
        recent_messages = history.messages[-n:]
        messages = []

        for msg in recent_messages:
            if hasattr(msg, "content") and msg.content:
                role = "KullanÄ±cÄ±" if hasattr(msg, "type") and msg.type == "human" else "PiriX"
                messages.append(f"{role}: {msg.content}")
            
        summary = summarize_messages(messages)
        if not summary:
            return ""

        return summary
    
    def create_enhanced_query(current_question, history):
        """Mevcut soru ile chat history'yi birleÅŸtirerek geliÅŸmiÅŸ sorgu oluÅŸtur"""
        summary = get_conversation_summary(history, n=5)
        
        if not summary:
            return current_question
        
        # Sadece Ã¶nemli context'i ekle, Ã§ok uzun olmasÄ±n
        enhanced_query = f"Ã–zet: {summary}\n\nSoru: {current_question}"
        print("Enhanced Query:", enhanced_query)
        return enhanced_query

    def retrieve(state: State, session_id: str = None):
        query = state["question"]
        
        # Chat history'yi al
        history = get_session_history(session_id) if session_id else ChatMessageHistory()
        
        # GeliÅŸmiÅŸ sorgu oluÅŸtur
        enhanced_query = create_enhanced_query(query, history)
        
        
        # Ä°lk aÅŸama: VektÃ¶r veritabanÄ±ndan benzer dokÃ¼manlarÄ± getir
        results = vector_store.similarity_search_with_score(enhanced_query, k=25)
        top_docs = [doc for doc, _ in results]
        
        # Ä°kinci aÅŸama: Cross-encoder ile dokÃ¼manlarÄ± yeniden sÄ±rala
        cross_encoder_inputs = [(query, doc.page_content) for doc in top_docs]
        scores = cross_encoder.predict(cross_encoder_inputs)
        
        # SonuÃ§larÄ± skorlarÄ±na gÃ¶re sÄ±rala ve en iyi 10'unu al
        reranked_docs = [
            doc for _, doc in sorted(zip(scores, top_docs), key=itemgetter(0), reverse=True)
        ][:10]

        return {
            "context": reranked_docs,
            "question": query
        }

    def generate(state: State, session_id: str):
        docs_content = "\n\n".join(doc.page_content for doc in state['context'])
        config = {"configurable": {"session_id": session_id}}

        history = get_session_history(session_id) if session_id else ChatMessageHistory()
        summary = get_conversation_summary(history, n=5)
        input_data = {
            "question": state["question"],
            "context": docs_content,
            "summary": summary
        }

        answer = chain_with_history.invoke(input_data, config=config)
        return {"answer": answer.content}

    return retrieve, generate